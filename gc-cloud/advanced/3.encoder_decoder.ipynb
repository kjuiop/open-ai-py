{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder_decoder architecture\n",
    "\n",
    "- 인코더-디코더 아키텍처는 시퀀스-시퀀스 아키텍처 이다.\n",
    "\n",
    "- 인코더-디코더 아키텍처는 입력 시퀀스를 입력받고 시퀀스를 출력한다.\n",
    "\n",
    "- 대규모 언어 모델에 전송되는 프롬프트를 형성하는 단어 시퀀스\n",
    "\n",
    "- 인코더 디코더는 2단계로 구성이 됨\n",
    "\n",
    "1. 입력 문장의 벡터 표현을 생성하는 인코더 단계\n",
    "\n",
    "- 순환 신경망 인코더는 입력 시퀀스의 각 토큰을 하나씩 가져와 이 토큰을 나타내는 상태를 생성\n",
    "\n",
    "- 모든 입력 토큰을 RNN 에 수집하는 작업이 끝나면 전체 입력 문장을 나타내는 벡터를 출력\n",
    "\n",
    "2. 인코더 단계에 이어 시퀀스 출력을 생성하는 디코더 단계\n",
    "\n",
    "- 디코더는 입력 문장의 벡터 표현을 취하고 해당 표현으로부터 출력 문장을 생성\n",
    "\n",
    "- RNN 디코더의 경우 단계적으로 수행하여 출력을 디코딩함\n",
    "\n",
    "<br />\n",
    "\n",
    "### 훈련 단계\n",
    "\n",
    "- 모델을 훈련하려면 데이터 세트가 필요함\n",
    "\n",
    "- 데이터 세트는 모델이 모방할 입력/출력 쌍의 모음을 가리킴\n",
    "\n",
    "- 데이터 세트를 모델에 공급하면 모델이 가중치를 교정함\n",
    "\n",
    "- 데이터 세트의 주어진 입력에 대해 생성된 오류를 기반으로 학습을 진행함\n",
    "\n",
    "<br />\n",
    "\n",
    "## 데이터 세트를 만드는 법\n",
    "\n",
    "- 입력 및 출력 텍스트 모음이 필요\n",
    "\n",
    "- 번역의 경우 한 문장은 소스 언어, 다른 문장은 타겟 언어로 되어 있는 문장의 쌍이 됨\n",
    "\n",
    "- 소스 언어 문장을 인코더에 입력한 후 디코더가 생성한 내용과 실제 번역 간의 오류를 계산함\n",
    "\n",
    "- 디코더에도 훈련 시점에는 자체 입력을 필요로 함\n",
    "\n",
    "- 디코더가 올바른 이전 토큰으로부터 다음 토큰을 생성하도록 하는 학습 방법을 교사 강제라 함\n",
    "\n",
    "- 탐욕적 탐색은 가장 높은 확률을 갖는 토큰을 생성하는 것\n",
    "\n",
    "- 빔 탐색은 디코더가 생성한 확률을 사용하여 개별 단어가 아닌 문장 덩어리의 확률들을 평가함\n",
    "\n",
    "- 각 단계에서 가장 생성될 가능성이 높은 청크를 보관\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "### 탐욕적 탐색과 빔 탐색\n",
    "\n",
    "- 탐욕적 탐색은 항상 가장 높은 확률을 가진 단어를 선택하는 반면, 빔 탐색은 여러 개의 가능한 단어를 고려하여 결합된 확률이 가장 높은 단어를 선택\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코더 디코더 아키텍처를 기반으로 문자 기반 텍스트 생성기 구현\n",
    "\n",
    "- 문자 기반이란 네트워크에서 소비되고 생성되는 토큰이 단어가 아닌 문자라는 것을 의미\n",
    "\n",
    "- 사람들의 대화는 문법적으로 항상 정확하지 않기 때문에 가장 가능성이 높은 문자만 예측하는 법을 배워야 함\n",
    "\n",
    "<br />\n",
    "\n",
    "### 인코더 디코더 아키텍처 구현 \n",
    "\n",
    "1. 필요한 라이브러리 (TensorFlow Keras) 를 아키텍처에 전달\n",
    "\n",
    "2. TF Keras Utils.get 파일을 사용하여 데이터 세트를 다운로드\n",
    "\n",
    "3. 데이터 세트가 디스크에 있으므로 이를 가변적인 cortex 에 로드\n",
    "\n",
    "4. 텍스트 변수에는 데이터세트에 있는 전체 문자열이 포함되게 됨\n",
    "\n",
    "5. 셀은 텍스트 데이터 세트에서 고유한 문자의 수를 계산하고, 이러한 문자는 신경망이 훈련하는 동안 소비하고 서비스 중에 생성하는 토큰이 됨\n",
    "\n",
    "6. 텍스트를 벡터화하는 작업이 진행됨\n",
    "\n",
    "- 실제 문자열 시퀀스에서 문자들을 추출할 때 TensorFlow 에서 TF 문자열 유니코드 분할을 사용하여 수행\n",
    "\n",
    "- 텍스트는 문자 시퀀스 목록으로 변환됨\n",
    "\n",
    "- 위의 65개 고유한 문자가 있으며, 이를 전달하면 문자에 해당하는 ID 를 생성하는 레이어가 생성되며, 그 레이어 안에서 문자와 ID 사이에 매핑이 이루어짐\n",
    "\n",
    "- 역 매핑을 얻으려면 동일한 문자열 조회 계층을 사용\n",
    "\n",
    "- get vocabulary 를 사용해 연초에 검색한 것과 정확히 동일한 어휘를 얻음\n",
    "\n",
    "7. 신경망을 훈련할 때 TF 데이터 셋 API 를 사용하는데, 이 API 는 수많은 슬라이스를 변환하는 방법을 제공\n",
    "\n",
    "- 인스턴스의 답변은 텍스트 전체 코퍼스를 나타내며 id 는 이를 데이터 세트에 저장함\n",
    "\n",
    "8. 입력 시퀀스의 다음 문자에 대한 시퀀스는 무엇입니까?\n",
    "\n",
    "- 원래 시퀀스를 가져와 해당 시퀀스를 잘라내어 입력 시퀀스를 생성합니다.\n",
    "\n",
    "- 마지막 문자와 시작 부분에서 첫 번째 문자를 추가하여 대상 시퀀스를 만듭니다.\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "### 모델을 만드는 방법\n",
    "\n",
    "1. 어휘 크기, 백터 크기 등 여러 변수를 설정, 뉴런이나 순환 계층의 갯수도 필요\n",
    "\n",
    "2. Keras 서브클래스 API 를 사용하여 이를 생성\n",
    "\n",
    "3. 내 모델이라는 새로운 클래스를 생성하고 여기서 TF Keras 모델의 하위 클래스를 만듬\n",
    "\n",
    "4. 생성자와 호출 함수라는 두 가지 함수만 재정의를 해서 사용\n",
    "\n",
    "<br />\n",
    "\n",
    "### 각 기능\n",
    "\n",
    "1. 모델의 하이퍼 매개변수, 어휘 크기, 임베딩 차원, 해당 개수의 뉴런 수를 사용\n",
    "\n",
    "- 순환 계층에 대한 뉴런의 수를 늘리고, 필요한 계층만 제한하여 클래스의 변수로 저장\n",
    "\n",
    "2. 문자의 정적표현은 우리가 컨텍스트에 따라 표현을 수정하여 반복 계층에 전달\n",
    "\n",
    "3. 순환 계층의 출력을 우리가 가지고 있는 숫자만큼 출력할 dense 계층에 전달\n",
    "\n",
    "4. 우리의 어휘는 가능한 65개의 문자 각각에 대한 한 가지 점수를 부여하고, 그 점수는 다음 문자가 될 확률을 나타냄\n",
    "\n",
    "\n",
    "<br />\n",
    "\n",
    "### 모델 훈련\n",
    "\n",
    "1. 모델을 훈련하기 전에 손실이 필요한데, 이는 모델의 출력과 실제 출력을 비교하는 손실 함수\n",
    "\n",
    "- 본질적으로 여러 클래스가 있는 분류 문제이며, 클래스는 각각 다음과 같습니다.\n",
    "\n",
    "- 다음이 될 수 있는 문자에 대한 손실을 희소하고 범주적인 교차 엔트로피 손실\n",
    "\n",
    "- 신경망 출력인 로직은 우리가 직접적으로 확률을 나타내는 것이 아니라 로직 점수에서 계산하도록 구성\n",
    "\n",
    "- 손실이 발생하면 모델을 결합할 수 있습니다. 기본적으로 손실과 최적화 도구를 모델에 연결한 것입니다.\n",
    "\n",
    "2. 데이터 세트에 대해 model.fit 을 실행합니다.\n",
    "\n",
    "- 훈련을 받고 싶은 여러 시대를 선택합니다.\n",
    "\n",
    "- 에포크는 데이터세트에 대한 전체 패스입니다.\n",
    "\n",
    "3. 벡터를 사용하고, 훈련 중에 가중치가 저장되도록 콜백을 제공합니다.\n",
    "\n",
    "- 디코더 아키텍처는 모델을 즉시 사용할 수 없고, 훈련된 모델을 사용하여 생성된 텍스트를 한 단계씩 디코딩하는 디코딩 함수를 사용해야 합니다.\n",
    "\n",
    "- 이를 TF Keras 모델에서 서브클래싱을 했습니다.\n",
    "\n",
    "- 초기 프롬프트 초기 인코더-디코더 모델이 완성하고 예측하고 새로운 문자를 생성하기를 원하는 문자 시퀀스입니다.\n",
    "\n",
    "- 입력을 우회하여 해당 텍스트를 일련의 문자로 변환하고 그 다음 일련의 문자로 구성된 문자열을 일련의 ID 로 변환합니다.\n",
    "\n",
    "4. ID 를 입력으로 받아서 예측된 로짓을 출력합니다.\n",
    "\n",
    "- 가장 가능성 있는 토큰 ,가장 가능성 있는 문자와 함께 이전에 본 내용을 요약한 상태가 필요합니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
